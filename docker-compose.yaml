networks:
  microservices-net:
    driver: bridge

services:
  
  postgres:
    image: postgres:16
    container_name: storage_postgres
    environment:
      POSTGRES_DB: storagedb
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5433:5432"
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
      - ./init:/docker-entrypoint-initdb.d
    networks:
      - microservices-net

  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghiklmnopqrstuvwxyz123456
    # volumes:
    #   - ./kafka_data:/bitnami/kafka
    networks:
      - microservices-net

  room-simulator-service:
    build: ./RoomSimulatorService
    container_name: room-simulator-service
    environment:
      simulation.pid-lstm.prediction-interval: 15
      simulation.pid-lstm.correction-gain: 0.9
      simulation.pid-lstm.max-setpoint-shift: 2
      feign.client.config.pid-controller.url: http://pid-controller-service:8081
      feign.client.config.storage-service.url: http://storage-service:8082
      feign.client.config.pid-autotuner.url: http://pid-autotune-servicet:7000
      feign.client.config.RL-trainer.url: http://rl-trainer-service:7002
      feign.client.config.AI-models.url: http://ai-inference-service:7003
    ports:
      - "8080:8080"
    networks:
      - microservices-net

  pid-controller-service:
    build: ./ControllerService
    container_name: pid-controller-service
    environment:
       feign.client.config.storage-service.url: http://storage-service:8082
    ports:
      - "8081:8081"
    networks:
      - microservices-net

  storage-service:
    build: ./StorageService
    container_name: storage-service
    depends_on: 
      - postgres
    environment:
       SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/storagedb
       SPRING_DATASOURCE_USERNAME: postgres
       SPRING_DATASOURCE_PASSWORD: postgres
    ports:
      - "8082:8082"
    networks:
      - microservices-net

  frontend:
    build: ./FrontEndDiplom/temperature-simulator
    container_name: frontend
    depends_on:
      - storage-service
      - room-simulator-service
#    volumes:
#      - "./nginx.conf:/etc/nginx/nginx.conf"
    ports:
      - "80:80"
    networks:
      - microservices-net

  pid-autotune-service:
    build: ./PID-autotune-service
    container_name: pid-autotune-service
    depends_on:
      - postgres
      - storage-service
      - room-simulator-service
    environment:
      - SIM_API=http://room-simulator-service:8080/api
      - STORE_API=http://storage-service:8082/api
      - SELF_HOST=http://pid-autotune-service:7000
    ports:
      - "7000:7000"
    networks:
      - microservices-net

  lstm-trainer-service:
    build: ./LSTM-trainer-service
    container_name: lstm-trainer-service
    depends_on:
      - postgres
      - storage-service
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=storagedb
      - DB_USER=postgres
      - DB_PASSWORD=postgres
      - STORE_API=http://storage-service:8082/api
      - MODELS_DIR=/models
    volumes:
      - ./models:/models
    ports:
      - "7001:7001"
    networks:
      - microservices-net

  rl-trainer-service:
    build: ./RL-trainer-service
    container_name: rl-trainer-service
    depends_on:
      - postgres
      - storage-service
      - room-simulator-service
    environment:
      - SIM_API=http://room-simulator-service:8080/api
      - STORE_API=http://storage-service:8082/api
      - MODELS_DIR=/models
      - TRAINER_TTL_SEC=1800
    volumes:
      - ./models:/models
    ports:
      - "7002:7002"
    networks:
      - microservices-net

  ai-inference-service:
    build: ./AI-inference-service
    container_name: ai-inference-service
    environment:
      - MODELS_DIR=/models
    volumes:
      - ./models:/models
    ports:
      - "7003:7003"
    networks:
      - microservices-net